{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RusVectōrēs: семантические модели для русского языка\n",
    "\n",
    "#### Елизавета Кузьменко, Андрей Кутузов\n",
    "\n",
    "В этом тьюториале мы рассмотрим возможности использования веб-сервиса RusVectōrēs и векторных семантических моделей, которые этот веб-сервис предоставляет пользователям. Наша задача -- от \"сырого\" текста (т.е. текста без всякой предварительной обработки) прийти к данным, которые мы можем передать векторной модели и получить от неё интересующий нас результат.\n",
    "\n",
    "Тьюториал состоит из трех частей:\n",
    "* в первой части мы научимся осуществлять предобработку текстовых файлов так, чтобы в дальнейшем они могли использованы в качестве входных данных для моделей RusVectōrēs.\n",
    "* во второй части мы научимся работать с векторными моделями и выполнять простые операции над векторами слов, такие как \"найти семантические аналоги\", \"сложить вектора двух слов\", \"вычислить коэффициент близости между двумя векторами слов\". \n",
    "* в третьей части мы научимся обращаться к сервису RusVectōrēs через API.\n",
    "\n",
    "Мы рекомендуем использовать **Python3**, работоспособность тьюториала для **Python2** не гарантируется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Предобработка текстовых данных\n",
    "\n",
    "Функциональность **RusVectōrēs** позволяет пользователям делать запрос к моделям [с одним конкретным словом](https://rusvectores.org/ru/similar/) или [с несколькими словами](https://rusvectores.org/ru/calculator/). С помощью сервиса можно также анализировать отношения между [бóльшим количеством слов](https://rusvectores.org/ru/visual/). Но что делать, если вы хотите обработать очень большую коллекцию текстов или ваша задача не решается при помощи конкретных единичных запросов к серверу, которые можно сделать вручную?\n",
    "\n",
    "В этом случае можно скачать одну из наших [моделей](https://rusvectores.org/ru/models/), а затем обрабатывать с её помощью тексты локально на вашем компьютере. Однако в этом случае необходимо, чтобы данные, которые подаются на вход модели, были в том же формате, что и данные, на которых эта модель была натренирована.\n",
    "\n",
    "Вы можете использовать наши готовые скрипты, чтобы из сырого текста получить текст в формате, который можно подавать на вход модели. Скрипты лежат [здесь](https://github.com/akutuzov/webvectors/tree/master/preprocessing). Как следует из их названия, один из скриптов использует для предобработки UDPipe, а другой Mystem. Оба скрипта используют стандартные потоки ввода и вывода, принимают на вход текст, выдают тот же текст, только лемматизированный и с частеречными тэгами. Если же вы хотите детально во всем разобраться и понять, например, в чем разница между UDPipe и Mystem, то читайте далее :)\n",
    "\n",
    "Предобработка текстов для тренировки моделей осуществлялась следующим образом:\n",
    "* лемматизация и удаление стоп-слов;\n",
    "* приведение лемм к нижнему регистру;\n",
    "* добавление частеречного тэга для каждого слова.\n",
    "\n",
    "Особого внимания заслуживает последний пункт предобработки. Как можно видеть из таблицы с описанием моделей, частеречные тэги для слов в различных моделях принадлежат к разным тагсетам. Первые модели использовали [тагсет **Mystem**](https://tech.yandex.ru/mystem/doc/grammemes-values-docpage/), затем мы перешли на [**Universal POS tags**](https://universaldependencies.org/u/pos/). В моделях на базе [**fastText**](https://fasttext.cc/) частеречные тэги не используются вовсе.\n",
    "\n",
    "Давайте попробуем воссоздать процесс предобработки текста на примере рассказа [О. Генри \"Русские соболя\"](https://rusvectores.org/static/henry_sobolya.txt). Для предобработки мы предлагаем использовать [*UDPipe*](https://ufal.mff.cuni.cz/udpipe), чтобы сразу получить частеречную разметку в виде Universal POS-tags. Сначала установим обертку *UDPipe* для Python:\n",
    "\n",
    "`pip install ufal.udpipe`\n",
    "\n",
    "*UDPipe* использует предобученные модели для лемматизации и тэггинга. Вы можете использовать [нашу модель](https://rusvectores.org/static/models/udpipe_syntagrus.model) или обучить свою. \n",
    "\n",
    "Чтобы загружать файлы, можно использовать реализацию wget в виде питоновской библиотеки:\n",
    "\n",
    "`pip install wget`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "udpipe_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "text_url = 'https://rusvectores.org/static/henry_sobolya.txt'\n",
    "\n",
    "modelfile = wget.download(udpipe_url)\n",
    "textfile = wget.download(text_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем модель *UDPipe*, читаем текстовый файл и обрабатываем его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline\n",
    "\n",
    "def tag_ud(text='Текст нужно передать функции в виде строки!', modelfile='udpipe_syntagrus.model'):\n",
    "    model = Model.load(modelfile)\n",
    "    pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "    processed = pipeline.process(text) # обрабатываем текст, получаем результат в формате conllu\n",
    "    output = [l for l in processed.split('\\n') if not l.startswith('#')] # пропускаем строки со служебной информацией\n",
    "    tagged = [w.split('\\t')[2].lower() + '_' + w.split('\\t')[3] for w in output if w] # извлекаем из обработанного текста лемму и тэг\n",
    "    tagged_propn = []\n",
    "    propn  = []\n",
    "    for t in tagged:\n",
    "        if t.endswith('PROPN'):\n",
    "            if propn:\n",
    "                propn.append(t)\n",
    "            else:\n",
    "                propn = [t]\n",
    "        else:\n",
    "            if len(propn) > 1:\n",
    "                name = '::'.join([x.split('_')[0] for x in propn]) + '_PROPN'\n",
    "                tagged_propn.append(name)\n",
    "            elif len(propn) == 1:\n",
    "                tagged_propn.append(propn[0])\n",
    "            tagged_propn.append(t)\n",
    "            propn = []\n",
    "    return tagged_propn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['русский_PROPN', 'соболь_NOUN', '._PUNCT', 'о.::генри_PROPN', 'когда_SCONJ', 'синий_ADJ', ',_PUNCT', 'как_SCONJ', 'ночь_NOUN', ',_PUNCT', 'глаз_NOUN', 'молли_VERB', 'мак-кивер_PROPN', 'класть_VERB', 'малыш::брэди_PROPN', 'на_ADP', 'оба_NUM', 'лопатка_NOUN', ',_PUNCT', 'он_PRON', 'вынужденный_ADJ', 'быть_AUX', 'покидать_VERB', 'ряд_NOUN', 'банда_NOUN', '«дымовый_ADJ', 'труба»_NOUN', '._PUNCT', 'таков_ADJ', 'власть_NOUN']\n"
     ]
    }
   ],
   "source": [
    "text = open(textfile, 'r', encoding='utf-8').read()\n",
    "processed_ud = tag_ud(text=text, modelfile=modelfile)\n",
    "print(processed_ud[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UDPipe позволяет нам распознавать имена собственные, и несколько идущих подряд имен мы можем склеить в одно.\n",
    "Вместо UDPipe возможно использовать и Mystem (удобнее использовать [pymystem](https://pypi.python.org/pypi/pymystem3) для Python), хотя Mystem имена собственные не распознает. Для того чтобы работать с последними моделями RusVectōrēs, понадобится сконвертировать тэги Mystem в UPOS.\n",
    "\n",
    "Сначала нужно установить библиотеку pymystem:\n",
    "\n",
    "`pip install pymystem3`\n",
    "\n",
    "Затем импортируем эту библиотеку и анализируем с её помощью текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "def tag_mystem(text='Текст нужно передать функции в виде строки!'):  \n",
    "    m = Mystem()\n",
    "    processed = m.analyze(text)\n",
    "    tagged = []\n",
    "    for w in processed:\n",
    "        try:\n",
    "            lemma = w[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            pos = w[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "            pos = pos.split('=')[0].strip()\n",
    "            tagged.append(lemma.lower() + '_' + pos)\n",
    "        except KeyError:\n",
    "            continue # я здесь пропускаю знаки препинания, но вы можете поступить по-другому\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['русский_S', 'соболь_S', 'о_PR', 'генри_S', 'когда_CONJ', 'синий_A', 'как_CONJ', 'ночь_S', 'глаз_S', 'молль_S']\n"
     ]
    }
   ],
   "source": [
    "processed_mystem = tag_mystem(text=text)\n",
    "print(processed_mystem[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, тэги Mystem отличаются от Universal POS-tags, поэтому следующим шагом должна быть их конвертация в Universal Tags. Вы можете воспользоваться вот [этой таблицей конвертации](https://github.com/akutuzov/universal-pos-tags/blob/4653e8a9154e93fe2f417c7fdb7a357b7d6ce333/ru-rnc.map):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'COM': 'ADJ', 'APRO': 'DET', 'PART': 'PART', 'PR': 'ADP', 'ADV': 'ADV', 'INTJ': 'INTJ', 'S': 'NOUN', 'V': 'VERB', 'CONJ': 'SCONJ', 'UNKN': 'X', 'ANUM': 'ADJ', 'NUM': 'NUM', 'NONLEX': 'X', 'SPRO': 'PRON', 'ADVPRO': 'ADV', 'A': 'ADJ'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/akutuzov/universal-pos-tags/4653e8a9154e93fe2f417c7fdb7a357b7d6ce333/ru-rnc.map'\n",
    "\n",
    "mapping = {}\n",
    "r = requests.get(url, stream=True)\n",
    "for pair in r.text.split('\\n'):\n",
    "    pair = re.sub('\\s+', ' ', pair, flags=re.U).split(' ')\n",
    "    if len(pair) > 1:\n",
    "        mapping[pair[0]] = pair[1]\n",
    "\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь усовершенствуем нашу функцию `tag_mystem`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_mystem(text='Текст нужно передать функции в виде строки!'):  \n",
    "    m = Mystem()\n",
    "    processed = m.analyze(text)\n",
    "    tagged = []\n",
    "    for w in processed:\n",
    "        try:\n",
    "            lemma = w[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            pos = w[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "            pos = pos.split('=')[0].strip()\n",
    "            if pos in mapping:\n",
    "                tagged.append(lemma + '_' + mapping[pos]) # здесь мы конвертируем тэги\n",
    "            else:\n",
    "                tagged.append(lemma + '_X') # на случай, если попадется тэг, которого нет в маппинге\n",
    "        except KeyError:\n",
    "            continue # я здесь пропускаю знаки препинания, но вы можете поступить по-другому\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['русский_NOUN', 'соболь_NOUN', 'о_ADP', 'генри_NOUN', 'когда_SCONJ', 'синий_ADJ', 'как_SCONJ', 'ночь_NOUN', 'глаз_NOUN', 'молль_NOUN']\n"
     ]
    }
   ],
   "source": [
    "processed_mystem = tag_mystem(text=text)\n",
    "print(processed_mystem[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, теперь частеречные тэги в тексте, проанализированном при помощи Mystem, сравнимы с тэгами Unversal POS (хотя сам анализ оказался разным)!\n",
    "\n",
    "В ходе обработки данных для тренировки моделей мы также удаляли стоп-слова: пунктуацию, служебные части речи, местоимения. Таким образом, наши модели не знают слов \"как\", \"мы\" и др. Вы также можете удалить стоп-слова, воспользовавшись, например, [списком стоп-слов в библиотеке NLTK](https://pythonspot.com/nltk-stop-words/).\n",
    "\n",
    "Итак, в ходе этой части тьюториала мы научились от \"сырого текста\" приходить к лемматизированному тексту с частеречными тэгами, который уже можно подавать на вход модели! Теперь приступим непосредственно к работе с векторными моделями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Работа с векторными моделями при помощи библиотеки Gensim\n",
    "\n",
    "Прежде чем переходить к работе непосредственно с **RusVectōrēs**, мы посмотрим на то, как работать с дистрибутивными моделями при помощи существующих библиотек. \n",
    "\n",
    "Для работы с эмбеддингами слов существуют различные библиотеки: [gensim](https://radimrehurek.com/gensim/), [keras](https://keras.io/), [tensorflow](https://www.tensorflow.org/). Мы будем работать с библиотекой *gensim*, ведь в основе нашего сервера именно она и используется.\n",
    "\n",
    "\n",
    "***Gensim***  - изначально библиотека для тематического моделирования текстов. Однако помимо различных алгоритмов для *topic modeling* в ней реализованы на python и алгоритмы из тулкита *word2vec* (который в оригинале был написан на C++). Прежде всего, если *gensim* у вас на компьютере не установлен, нужно его установить:\n",
    "\n",
    "`pip install gensim`\n",
    "\n",
    "Gensim регулярно обновляется, так что не будет лишним удостовериться, что у вас установлена последняя версия, а при необходимости проапдейтить библиотеку:\n",
    "\n",
    "`pip install gensim --upgrade` \n",
    "\n",
    "или \n",
    "\n",
    "`pip install gensim -U`\n",
    "\n",
    "При подготовке этого тьюториала использовался *gensim* версии 3.4.0.\n",
    "\n",
    "Поскольку обучение и загрузка моделей могут занимать продолжительное время, иногда бывает полезно вести лог событий. Для этого используется стандартная питоновская библиотека `logging`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа с моделью\n",
    "\n",
    "Для своих индивидуальных нужд и экспериментов бывает полезно самому натренировать модель на нужных данных и с нужными параметрами. Но для каких-то общих целей модели уже есть как для русского языка, так и для английского.\n",
    "\n",
    "Модели для русского скачать можно здесь - https://rusvectores.org/ru/models/\n",
    "\n",
    "Скачаем модель для русского языка, созданную на основе [Национального корпуса русского языка (НКРЯ)](http://www.ruscorpora.ru/). Вы можете найти её по [этой ссылке](https://rusvectores.org/static/models/rusvectores4/RNC/ruscorpora_upos_skipgram_300_5_2018.vec.gz).\n",
    "\n",
    "Существуют несколько форматов, в которых могут храниться модели. Во-первых, данные могут храниться в нативном формате *word2vec*, при этом модель может быть бинарной или не бинарной. Для загрузки модели в формате *word2vec* в классе `KeyedVectors` (в котором хранится большинство относящихся к дистрибутивным моделям функций) существует функция `load_word2vec_format`, а бинарность модели можно указать в аргументе `binary` (внизу будет пример). Помимо этого, модель можно хранить и в собственном формате *gensim*, для этого существует класс `Word2Vec` с функцией `load`.\n",
    "\n",
    "Поскольку модели бывают разных форматов, то для них написаны разные функции загрузки; бывает полезно учитывать это в своем скрипте. Наш код определяет тип модели по её расширению, но вообще файл с моделью может называться как угодно, жестких ограничений для расширения нет. Наша модель, которую мы загрузили с сайта, хранится в небинарном формате word2vec, при этом она сжата при помощи `gzip` (расширение `.gz`), и *gensim* умеет также загружать сжатые модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-10 22:11:01,499 : INFO : loading projection weights from ruscorpora_upos_skipgram_300_5_2018.vec.gz\n",
      "2018-05-10 22:13:10,889 : INFO : loaded (195071, 300) matrix from ruscorpora_upos_skipgram_300_5_2018.vec.gz\n"
     ]
    }
   ],
   "source": [
    "model_url = 'https://rusvectores.org/static/models/rusvectores4/RNC/ruscorpora_upos_skipgram_300_5_2018.vec.gz'\n",
    "modelfile = wget.download(model_url)\n",
    "m = 'ruscorpora_upos_skipgram_300_5_2018.vec.gz'\n",
    "if m.endswith('.vec.gz'):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(m, binary=False)\n",
    "elif m.endswith('.bin.gz'):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(m, binary=True)\n",
    "else:\n",
    "    model = gensim.models.Word2Vec.load(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для моделей `fasttext` существует отдельный класс и отдельная функция загрузки:\n",
    "\n",
    "`gensim.models.fasttext.FastText.load()`\n",
    "\n",
    "Следует иметь в виду, что модели `fasttext` включают в себя несколько файлов, и библиотека не умеет самостоятельно определять, какой из нескольких файлов в архиве необходимо загружать. Следовательно, перед загрузкой скачанный архив с моделью **необходимо распаковать**. Вручную определить необходимый для загрузки файл несложно, чаще всего это файл с расширением `.model` (остальные файлы из архива должны быть в той же папке). Всё это может привести к некоторой путанице (непонятно, когда нужно распаковывать, а когда загружать так), поэтому можно просто запомнить, что модели `fasttext` не такие, как все :)\n",
    "\n",
    "Существует и альтернативный способ закрузки моделей. Так, некоторые модели можно загрузить прямо из *gensim*, используя метод `gensim.downloader` и репозиторий [**gensim-data**](https://github.com/RaRe-Technologies/gensim-data). В таблице по ссылке можно найти доступные модели и другие ресурсы; наша модель, обученная на НКРЯ, находится под идентификатором `word2vec-ruscorpora-300`. В дальнейшем планируется добавить в репозиторий и другие модели. Репозиторий **gensim-data** регулярно обновляется, однако если вы хотите работать с most up-to-date моделью, лучше всего скачать её с **RusVectōrēs**.\n",
    "\n",
    "Загрузка моделей при помощи загрузчика gensim выглядит следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-10 22:13:17,155 : INFO : loading projection weights from /home/lizaku/gensim-data/word2vec-ruscorpora-300/word2vec-ruscorpora-300.gz\n",
      "2018-05-10 22:13:40,775 : INFO : loaded (184973, 300) matrix from /home/lizaku/gensim-data/word2vec-ruscorpora-300/word2vec-ruscorpora-300.gz\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "ruscorpora_model = api.load(\"word2vec-ruscorpora-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но вернемся к нашей модели, загруженной вручную. Мы можем нормализовать загруженные векторы, чтобы модель занимала меньше места в оперативной памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-10 22:13:48,398 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скажем, нам интересны такие слова (пример для русского языка):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = ['день_NOUN', 'ночь_NOUN', 'человек_NOUN', 'семантика_NOUN', 'студент_NOUN', 'студент_ADJ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попросим у модели 10 ближайших соседей для каждого слова и коэффициент косинусной близости для каждого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "день_NOUN\n",
      "день_PROPN 0.6864385604858398\n",
      "неделя_NOUN 0.6757771372795105\n",
      "утро_NOUN 0.6553983688354492\n",
      "днемя_NOUN 0.6455298066139221\n",
      "месяц_NOUN 0.6271342039108276\n",
      "вечер_NOUN 0.6199158430099487\n",
      "воскресенье_NOUN 0.6103948354721069\n",
      "лабель_PROPN 0.6027745604515076\n",
      "днямя_NOUN 0.5987443923950195\n",
      "днями_NOUN 0.5916012525558472\n",
      "\n",
      "\n",
      "ночь_NOUN\n",
      "ночь_PROPN 0.8277530670166016\n",
      "вечер_NOUN 0.7208157777786255\n",
      "рассвет_NOUN 0.7120692729949951\n",
      "ночь_ADV 0.677560031414032\n",
      "утро_NOUN 0.6737046241760254\n",
      "полночь_NOUN 0.6684714555740356\n",
      "днемя_NOUN 0.6066733002662659\n",
      "ночной_ADJ 0.606225848197937\n",
      "ноченька_NOUN 0.6043546199798584\n",
      "ночью_NOUN 0.5991744995117188\n",
      "\n",
      "\n",
      "человек_NOUN\n",
      "человек_PROPN 0.7707481384277344\n",
      "человекомя_NOUN 0.6712430715560913\n",
      "человек-с_NOUN 0.6292698383331299\n",
      "людей_NOUN 0.596764087677002\n",
      "людямя_NOUN 0.5873969197273254\n",
      "женщина_NOUN 0.5705885887145996\n",
      "человеколо_NOUN 0.5606503486633301\n",
      "человека_NOUN 0.5553063154220581\n",
      "поживший_VERB 0.5501028895378113\n",
      "людьми_NOUN 0.5424647331237793\n",
      "\n",
      "\n",
      "семантика_NOUN\n",
      "семантический_ADJ 0.766370415687561\n",
      "синтаксический_ADJ 0.7299877405166626\n",
      "метаязык_NOUN 0.7288526892662048\n",
      "синтактика_NOUN 0.7154618501663208\n",
      "референция_NOUN 0.7111104726791382\n",
      "ноэма_NOUN 0.7066413760185242\n",
      "синонимие_NOUN 0.7056178450584412\n",
      "декаузатив_NOUN 0.7039096355438232\n",
      "семантическить_ADJ 0.7029540538787842\n",
      "ноэтичесок_ADV 0.7006590366363525\n",
      "\n",
      "\n",
      "студент_NOUN\n",
      "аспирант_NOUN 0.7101088762283325\n",
      "преподаватель_NOUN 0.6825631260871887\n",
      "студентка_NOUN 0.6781647205352783\n",
      "заочник_NOUN 0.6675980687141418\n",
      "университет_NOUN 0.6635211706161499\n",
      "первокурсник_NOUN 0.6580964922904968\n",
      "вольнослушатель_NOUN 0.6541134715080261\n",
      "дипломник_NOUN 0.6477440595626831\n",
      "вечерник_NOUN 0.6436547040939331\n",
      "магистрант_NOUN 0.6360849142074585\n",
      "\n",
      "\n",
      "студент_ADJ is not present in the model\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    # есть ли слово в модели? Может быть, и нет\n",
    "    if word in model:\n",
    "        print(word)\n",
    "        # выдаем 10 ближайших соседей слова:\n",
    "        for i in model.most_similar(positive=[word], topn=10):\n",
    "            # слово + коэффициент косинусной близости\n",
    "            print(i[0], i[1])\n",
    "        print('\\n')\n",
    "    else:\n",
    "        # Увы!\n",
    "        print(word + ' is not present in the model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим косинусную близость пары слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3423996332495893\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('человек_NOUN', 'обезьяна_NOUN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найди лишнее!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "картофель_NOUN\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match('яблоко_NOUN груша_NOUN виноград_NOUN банан_NOUN лимон_NOUN картофель_NOUN'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реши пропорцию!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "чипсы_NOUN\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['пицца_NOUN', 'россия_NOUN'], negative=['италия_NOUN'])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Использование API сервиса RusVectōrēs\n",
    "\n",
    "Помимо локального использования модели, вы можете также обратиться к RusVectōrēs через API, чтобы использовать наши модели в автоматическом режиме, не скачивая их (скажем, из ваших скриптов). В нашем API имеется две функции:\n",
    "\n",
    "* получение списка семантически близких слов для заданного слова в заданной модели;\n",
    "* вычисление коэффициента косинусной близости между парой слов в заданной модели.\n",
    "\n",
    "Для того чтобы получить список семантически близких слов, необходимо выполнить GET-запрос по адресу в следующем формате:\n",
    "\n",
    "`https://rusvectores.org/MODEL/WORD/api/FORMAT/`\n",
    "\n",
    "Разберемся с компонентами этого запроса. `MODEL` - идентификатор модели, к которой мы хотим обратиться. Идентификаторы можно посмотреть в [таблице](https://rusvectores.org/ru/models/) со всеми моделями нашего сервиса. `WORD` - слово, для которого мы хотим узнать соседей. Следует помнить, что частеречный тэг здесь тоже нужен (точнее, вы можете отправлять запросы и без него, но тогда части речи ваших слов сервер определит автоматически - и не всегда правильно). `FORMAT` - формат выходных данных, в настоящий момент это *csv* (с разделением через табуляцию) либо *json*.\n",
    "\n",
    "Попробуем узнать семантических соседей для первых слов из нашего рассказа. Сначала создадим переменные с параметрами нашего запроса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['русский_PROPN', 'соболь_NOUN', '._PUNCT', 'о.::генри_PROPN', 'когда_SCONJ', 'синий_ADJ', ',_PUNCT', 'как_SCONJ', 'ночь_NOUN', ',_PUNCT', 'глаз_NOUN', 'Молли_VERB', 'Мак-Кивер_PROPN', 'класть_VERB', 'малыш::Брэди_PROPN']\n"
     ]
    }
   ],
   "source": [
    "print(processed_ud[:15])\n",
    "MODEL = 'ruscorpora_upos_skipgram_300_5_2018'\n",
    "FORMAT = 'csv'\n",
    "WORD = processed_ud[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def api_neighbor(m, w, f):\n",
    "    neighbors = {}\n",
    "    url = '/'.join(['http://rusvectores.org', m, w, 'api', f]) + '/'\n",
    "    r = requests.get(url=url, stream=True)\n",
    "    for line in r.text.split('\\n'):\n",
    "        try: # первые две строки в файле -- служебные, их мы пропустим\n",
    "            word, sim = re.split('\\s+', line) # разбиваем строку по одному или более пробелам\n",
    "            neighbors[word] = sim\n",
    "        except:\n",
    "            continue\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'чернобурый_ADJ': '0.6514838337898254', 'горностай_NOUN': '0.7106404304504395', 'выдр_NOUN': '0.6077498197555542', 'лисица_NOUN': '0.6174986362457275', 'собольий_ADJ': '0.6177839636802673', 'куница_NOUN': '0.6861780881881714', 'горностай_VERB': '0.6484810709953308', 'соболий_ADJ': '0.6293830871582031', 'горностай_ADJ': '0.650195300579071', 'куний_ADJ': '0.6200308799743652'}\n"
     ]
    }
   ],
   "source": [
    "print(api_neighbor(MODEL, WORD, FORMAT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API по умолчанию сообщает 10 ближайших соседей, изменить это количество в данный момент возможности нет.\n",
    "\n",
    "Теперь рассмотрим вторую функцию, доступную в API - вычисление коэффициента близости между двумя словами.\n",
    "Запросы для неё должны выполняться в таком виде:\n",
    "\n",
    "`https://rusvectores.org/MODEL/WORD1__WORD2/api/similarity/`\n",
    "\n",
    "Здесь переменные - `MODEL` (идентификатор модели, к которой мы обращаемся) и два слова (вместе с их частеречными тэгами). Обратите внимание, что слова разделены **двумя нижними подчеркиваниями**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def api_similarity(m, w1, w2):\n",
    "    url = '/'.join(['https://rusvectores.org', m, w1 + '__' + w2, 'api', 'similarity/'])\n",
    "    r = requests.get(url, stream=True)\n",
    "    return r.text.split('\\t')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3407696995115949'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = 'news_upos_cbow_600_2_2018'\n",
    "api_similarity(MODEL, WORD, 'мех_NOUN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом тьюториале мы научились обрабатывать тексты таким образом, чтобы их можно было отдавать в качестве входных данных моделям RusVectōrēs. Мы также рассмотрели основные операции над векторами слов в дистрибутивных семантических моделях и научились обращаться к сервису через API. Надеемся, что данный тьюториал подготовил вас к работе над вашими данными и к новым открытиям, которые можно совершить при помощи дистрибутивной семантики :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
